{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69L7Xv-C9F5z",
        "outputId": "00e9b09c-2f3e-4952-9dcd-4bf6c42bfe73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jy49-St-9cEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f7739f8-3c63-4b5f-84a3-f04cbe69ed70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 31.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 19.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.16+pt113cu116\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-cluster) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-cluster) (1.21.6)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0+pt113cu116\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_spline_conv-1.2.1%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (873 kB)\n",
            "\u001b[K     |████████████████████████████████| 873 kB 31.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 28.7 MB/s \n",
            "\u001b[?25hCollecting pymatgen\n",
            "  Downloading pymatgen-2022.11.7.tar.gz (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 62.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting pybtex\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 79.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from pymatgen) (0.8.10)\n",
            "Requirement already satisfied: palettable>=3.1.1 in /usr/local/lib/python3.8/dist-packages (from pymatgen) (3.3.0)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.8/dist-packages (from pymatgen) (5.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from pymatgen) (2.8.8)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.8/dist-packages (from pymatgen) (3.2.2)\n",
            "Collecting ruamel.yaml>=0.17.0\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting spglib>=2.0.2\n",
            "  Downloading spglib-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (515 kB)\n",
            "\u001b[K     |████████████████████████████████| 515 kB 79.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from pymatgen) (1.7.1)\n",
            "Collecting monty>=3.0.2\n",
            "  Downloading monty-2022.9.9-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting uncertainties>=3.1.4\n",
            "  Downloading uncertainties-3.1.7-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from pymatgen) (1.3.5)\n",
            "Collecting mp-api>=0.27.3\n",
            "  Downloading mp_api-0.30.5-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5->pymatgen) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5->pymatgen) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5->pymatgen) (1.4.4)\n",
            "Collecting emmet-core>=0.36.4\n",
            "  Downloading emmet_core-0.39.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from mp-api>=0.27.3->pymatgen) (1.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.8/dist-packages (from mp-api>=0.27.3->pymatgen) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from mp-api>=0.27.3->pymatgen) (57.4.0)\n",
            "Requirement already satisfied: pydantic>=1.10.2 in /usr/local/lib/python3.8/dist-packages (from emmet-core>=0.36.4->mp-api>=0.27.3->pymatgen) (1.10.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.5.0->pymatgen) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.5.0->pymatgen) (8.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.8/dist-packages (from pybtex->pymatgen) (6.0)\n",
            "Collecting latexcodec>=1.0.4\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (555 kB)\n",
            "\u001b[K     |████████████████████████████████| 555 kB 77.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from uncertainties>=3.1.4->pymatgen) (0.16.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchaudio) (1.13.0+cu116)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 80.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 61.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->pymatgen) (2022.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->pymatgen) (1.2.1)\n",
            "Building wheels for collected packages: torch-geometric, pymatgen\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=00fd0e680685e528b22bc3de5a90e872f20f09926c335410d03be57a45042ac3\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "  Building wheel for pymatgen (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymatgen: filename=pymatgen-2022.11.7-cp38-cp38-linux_x86_64.whl size=3904355 sha256=f45415009968533ac9568a5d0c941cf3ced46eadf94751828289ba01c6c046c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ef/92/89b7bac9d4bc6890c4aadf9d24864b44f6f19ca6d6d98237fb\n",
            "Successfully built torch-geometric pymatgen\n",
            "Installing collected packages: latexcodec, spglib, pybtex, monty, ruamel.yaml.clib, emmet-core, uncertainties, torchmetrics, tensorboardX, ruamel.yaml, psutil, mp-api, lightning-utilities, torch-geometric, pytorch-lightning, pymatgen\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed emmet-core-0.39.6 latexcodec-2.0.1 lightning-utilities-0.5.0 monty-2022.9.9 mp-api-0.30.5 psutil-5.9.4 pybtex-0.24.0 pymatgen-2022.11.7 pytorch-lightning-1.8.6 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 spglib-2.0.2 tensorboardX-2.5.1 torch-geometric-2.2.0 torchmetrics-0.11.0 uncertainties-3.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "pytorch_version = f\"torch-{torch.__version__}.html\"\n",
        "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/$pytorch_version\n",
        "!pip install torch-geometric pymatgen tqdm torchaudio torchvision pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz5I1Lyo-XrD"
      },
      "source": [
        "## 1. Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ypm5PI-U-Obj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import numpy as np \n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.data import Dataset, Data\n",
        "from torch.nn import Embedding, ModuleList\n",
        "from pymatgen.ext.matproj import MPRester\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9eORCsde-Qoj"
      },
      "outputs": [],
      "source": [
        "class GaussianFilter(object):\n",
        "    def __init__(self, start=0.1, stop=5.0, n_points=100, sigma=0.5):\n",
        "        self.start = start\n",
        "        self.stop = stop\n",
        "        self.n_points = n_points\n",
        "        self.sigma = sigma\n",
        "    def __call__(self, feature: torch.Tensor) -> torch.Tensor:\n",
        "        feature = feature.view(len(feature), 1)\n",
        "        r_0 = torch.linspace(self.start, self.stop, self.n_points).repeat(feature.shape[0], 1)\n",
        "        return torch.exp(-(feature - r_0)/self.sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q70GmIiq-SQZ"
      },
      "outputs": [],
      "source": [
        "class CrystalDataset(Dataset):\n",
        "    def __init__(self, root, api_key, species=None, mp_ids=None, cutoff=5.0, \n",
        "                 targets=['band_gap'], global_attrs=['volume', 'spacegroup'], node_features=None, \n",
        "                 filter=None, embeddings=None, transform=None, pre_transform=None):\n",
        "        \"\"\"\n",
        "        root = Where the dataset should be stored. This folder is split\n",
        "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
        "        \"\"\"\n",
        "        assert (species is not None) or (mp_ids is not None)\n",
        "        self.cutoff = cutoff\n",
        "        self.species = species\n",
        "        self.api_key = api_key\n",
        "        self.targets = targets\n",
        "        self.filter = filter\n",
        "        self.node_features = node_features\n",
        "        self.mp_ids = mp_ids\n",
        "        self.global_attrs = global_attrs\n",
        "        self.embeddings = embeddings\n",
        "        super(CrystalDataset, self).__init__(root, transform, pre_transform)\n",
        "        \n",
        "        \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        \"\"\" If this file exists in raw_dir, the download is not triggered.  \n",
        "        \"\"\"\n",
        "        return ['structures.dump', 'vasp_data.dump']\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        \"\"\" If these files are found in processed_dir, processing is skipped\"\"\"\n",
        "        with open(self.raw_paths[0], 'rb') as f:\n",
        "            structures = pickle.load(f)\n",
        "        with open(self.raw_paths[1], 'rb') as f:\n",
        "            vasp_data = pickle.load(f)\n",
        "        self.data = [structures, vasp_data]\n",
        "        return [f'data_{idx}.pt' for idx in range(len(self.data[0]))]\n",
        "        #return 'not_implemented.pt'\n",
        "\n",
        "    def download(self):\n",
        "        rester = MPRester(self.api_key)\n",
        "        if self.mp_ids is None:\n",
        "            assert self.species is not None\n",
        "            entries = rester.get_entries_in_chemsys(self.species)\n",
        "            entries_ids = [entry.entry_id for entry in entries]\n",
        "        else:\n",
        "            if isinstance(self.mp_ids, pd.DataFrame):\n",
        "                entries_ids_raw = list(self.mp_ids[0].values)\n",
        "            else:\n",
        "                entries_ids_raw = list(self.mp_ids)\n",
        "            entries_ids = [rester.get_materials_id_from_task_id(id) for id in tqdm(entries_ids_raw)]\n",
        "        structures = []\n",
        "        vasp_data = []\n",
        "        for id in tqdm(entries_ids):\n",
        "            vasp_data.append(rester.get_data(id))\n",
        "            structures.append(rester.get_structure_by_material_id(id))\n",
        "        with open(self.raw_paths[0], 'wb') as f:\n",
        "            pickle.dump(structures, f)\n",
        "        with open(self.raw_paths[1], 'wb') as f:\n",
        "            pickle.dump(vasp_data, f)\n",
        "        self.data = [structures, vasp_data]\n",
        "\n",
        "    def process(self):\n",
        "        with open(self.raw_paths[0], 'rb') as f:\n",
        "            structures = pickle.load(f)\n",
        "        with open(self.raw_paths[1], 'rb') as f:\n",
        "            vasp_data = pickle.load(f)\n",
        "        self.data = [structures, vasp_data]\n",
        "        for index, structure, vdata in tqdm(zip(range(len(structures)), structures, vasp_data), total=len(structures)):\n",
        "            # Get node features\n",
        "            node_feats = self._get_node_features(structure)\n",
        "            # Get edge features\n",
        "            edge_feats = self._get_edge_features(structure)\n",
        "            # Get adjacency info\n",
        "            edge_index = self._get_adjacency_info(structure)\n",
        "            # Get labels info\n",
        "            labels = self._get_labels(vdata)\n",
        "            # Get global info\n",
        "            global_feats = self._get_global_features(structure)\n",
        "\n",
        "            # Create data object\n",
        "            data = Data(x=node_feats, \n",
        "                        edge_index=edge_index,\n",
        "                        edge_attr=edge_feats,\n",
        "                        y=labels,\n",
        "                        global_attr=global_feats\n",
        "                        ) \n",
        "            torch.save(data, \n",
        "                    os.path.join(self.processed_dir, \n",
        "                                 f'data_{index}.pt'))\n",
        "\n",
        "    def _get_node_features(self, structure):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of Nodes, Node Feature size]\n",
        "        \"\"\"\n",
        "        all_node_feats = []\n",
        "\n",
        "        for element in structure.species:\n",
        "            node_feats = []\n",
        "            if self.node_features is None:\n",
        "                # Feature 1: Atomic number\n",
        "                if self.embeddings is not None:\n",
        "                    node_feats.extend(self.embeddings[0](torch.tensor(element.Z)))\n",
        "                else:\n",
        "                    node_feats.append(element.Z)                \n",
        "                # Feature 2: Atom radius\n",
        "                node_feats.append(element.atomic_radius)\n",
        "                # Feature 3: Average ionic radius\n",
        "                node_feats.append(element.average_ionic_radius)\n",
        "                # Feature 4: Ionization energy\n",
        "                node_feats.append(element.ionization_energy)\n",
        "                # Feature 5: Electronegativity\n",
        "                node_feats.append(element.X)\n",
        "                # Feature 6: Mendeleev number\n",
        "                node_feats.append(element.mendeleev_no)\n",
        "                # Feature 7: Electron affinity\n",
        "                node_feats.append(element.electron_affinity)\n",
        "                # Feature 8: Min oxidation state\n",
        "                node_feats.append(element.min_oxidation_state)\n",
        "                # Feature 9: Max oxitaion state\n",
        "                node_feats.append(element.max_oxidation_state)\n",
        "            else:\n",
        "                node_feats = []\n",
        "                if self.embeddings is not None and 'Z' in self.node_features:\n",
        "                    node_feats.extend(self.embeddings[0](torch.tensor(element.Z)))\n",
        "                    node_feats.extend([getattr(element, attr) for attr in self.node_features if attr is not 'Z'])\n",
        "                else:\n",
        "                    node_feats.extend([getattr(element, attr) for attr in self.node_features])\n",
        "\n",
        "            # Append node features to matrix\n",
        "            all_node_feats.append(node_feats)\n",
        "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
        "\n",
        "\n",
        "    def _get_edge_features(self, structure):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of edges, Edge Feature size]\n",
        "        \"\"\"\n",
        "        \n",
        "        all_edge_feats = []\n",
        "        features = torch.tensor(structure.get_neighbor_list(self.cutoff)[-1], dtype=torch.float)\n",
        "        if self.filter is not None:\n",
        "            features = self.filter(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _get_adjacency_info(self, structure):\n",
        "        \"\"\"\n",
        "        This will return an adjacency matrix in COO format / 2d array of the shape\n",
        "        [2, Number of edges]\n",
        "        \"\"\"\n",
        "        edge_indices = torch.tensor(structure.get_neighbor_list(self.cutoff)[:2])\n",
        "        return edge_indices\n",
        "\n",
        "    def _get_labels(self, label):\n",
        "        assert isinstance(self.targets, (tuple, list))\n",
        "        assert np.all([item in label[-1] for item in self.targets])\n",
        "        return torch.tensor([label[-1][item] for item in self.targets], dtype=torch.float32)\n",
        "\n",
        "    def _get_global_features(self, structure):\n",
        "        global_info = []\n",
        "        for attr in self.global_attrs:\n",
        "            assert attr in ['volume', 'spacegroup']\n",
        "            if attr == 'volume':\n",
        "                global_info.append(structure.volume)\n",
        "            if attr == 'spacegroup':\n",
        "                spg = structure.get_space_group_info()[1]\n",
        "                if self.embeddings is not None:\n",
        "                    global_info.extend(self.embeddings[1](torch.tensor(spg)))\n",
        "                else:\n",
        "                    global_info.append(spg)\n",
        "        return torch.tensor(global_info, dtype=torch.float).view(1, len(global_info)).contiguous()\n",
        "            \n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
        "            - Is not needed for PyG's InMemoryDataset\n",
        "        \"\"\"\n",
        "        data = torch.load(os.path.join(self.processed_dir, \n",
        "                                 f'data_{idx}.pt'))\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pHkAt1K-UND",
        "outputId": "49bcb0c0-7129-491c-ad2a-84fdfa7b8c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3402/3402 [07:51<00:00,  7.21it/s]\n",
            "100%|██████████| 3402/3402 [16:14<00:00,  3.49it/s]\n",
            "Processing...\n",
            "100%|██████████| 3402/3402 [00:54<00:00, 62.91it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the following lines if you want to manually download the data and convert it to the Dataset\n",
        "\n",
        "mp_ids = pd.read_csv('https://raw.githubusercontent.com/txie-93/cgcnn/master/data/material-data/mp-ids-3402.csv', header=None)[0].values\n",
        "root = 'gdrive/MyDrive/Colab Notebooks/GNN/data/MP3402'\n",
        "API_KEY = '2Uihe3wfq5ac6tMF'\n",
        "filter = GaussianFilter(start=0.5, stop=5.0, n_points=100)\n",
        "embeddings = ModuleList([Embedding(95, 16), Embedding(230, 32)])\n",
        "dataset = CrystalDataset(root=root, api_key=API_KEY, filter=filter, mp_ids=mp_ids, embeddings=embeddings)\n",
        "dataset.num_global_features = dataset[0].global_attr.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "969TwEg-Iia5"
      },
      "outputs": [],
      "source": [
        "# Use this line to continue with already prepared Dataset\n",
        "\n",
        "# dataset = torch.load('gdrive/MyDrive/Colab Notebooks/crystal_dataset.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVaBJIS0Isq2",
        "outputId": "56da93ab-0bd6-44a5-b598-b07777e801ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrystalDataset(3402)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBqIl0Py-VsK",
        "outputId": "d660a224-5376-4d94-bc56-0c9303c22ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of entries: 3402\n",
            "Number of node features: 24\n",
            "Number of edge features: 100\n",
            "Number of global features: 33\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of entries: {len(dataset)}\")\n",
        "print(f'Number of node features: {dataset.num_node_features}')\n",
        "print(f'Number of edge features: {dataset.num_edge_features}')\n",
        "print(f'Number of global features: {dataset.num_global_features}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1GsNopA_HYk"
      },
      "source": [
        "## 2. Building the MegNet block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qVH8gnh1_MzF"
      },
      "outputs": [],
      "source": [
        "from typing import Union, Tuple\n",
        "from torch_geometric.typing import PairTensor, Adj, OptTensor, Size, SparseTensor\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "\n",
        "class MegNetBlock(MessagePassing):\n",
        "    def __init__(self, node_channels: Tuple, edge_channels: Tuple, global_channels: Tuple,\n",
        "                 aggr: str = 'mean', batch_norm: bool = True, bias: bool = True, **kwargs):\n",
        "        super(MegNetBlock, self).__init__(aggr=aggr, **kwargs)\n",
        "        self.node_channels = node_channels\n",
        "        self.edge_channels = edge_channels\n",
        "        self.global_channels = global_channels\n",
        "        self.batch_norm = batch_norm\n",
        "\n",
        "        self.node_lin = ModuleList([Linear(node_channels[0], node_channels[1], bias=bias), Linear(node_channels[1], node_channels[2], bias=bias)])\n",
        "        self.edge_lin = ModuleList([Linear(edge_channels[0], edge_channels[1], bias=bias), Linear(edge_channels[1], edge_channels[2], bias=bias)])\n",
        "        self.global_lin = ModuleList([Linear(global_channels[0], global_channels[1], bias=bias), Linear(global_channels[1], global_channels[2], bias=bias)])\n",
        "\n",
        "        self.phi_edge = ModuleList([Linear(2*node_channels[2]+edge_channels[2]+global_channels[2], 2*node_channels[2]+edge_channels[2]+global_channels[2], bias=bias),\n",
        "                                    Linear(2*node_channels[2]+edge_channels[2]+global_channels[2], 2*node_channels[2]+edge_channels[2]+global_channels[2], bias=bias), \n",
        "                                    Linear(2*node_channels[2]+edge_channels[2]+global_channels[2], edge_channels[2], bias=bias)])\n",
        "        self.phi_node = ModuleList([Linear(edge_channels[2]+node_channels[2]+global_channels[2], edge_channels[2]+node_channels[2]+global_channels[2], bias=bias),\n",
        "                                    Linear(edge_channels[2]+node_channels[2]+global_channels[2], edge_channels[2]+node_channels[2]+global_channels[2], bias=bias), \n",
        "                                    Linear(edge_channels[2]+node_channels[2]+global_channels[2], node_channels[2], bias=bias)])\n",
        "        self.phi_global = ModuleList([Linear(node_channels[2]+edge_channels[2]+global_channels[2], node_channels[2]+edge_channels[2]+global_channels[2], bias=bias),\n",
        "                                      Linear(node_channels[2]+edge_channels[2]+global_channels[2], node_channels[2]+edge_channels[2]+global_channels[2], bias=bias),\n",
        "                                      Linear(node_channels[2]+edge_channels[2]+global_channels[2], global_channels[2], bias=bias)])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for module in self.node_lin:\n",
        "            module.reset_parameters()\n",
        "        for module in self.edge_lin:\n",
        "            module.reset_parameters()\n",
        "        for module in self.global_lin:\n",
        "            module.reset_parameters()\n",
        "        for module in self.phi_edge:\n",
        "            module.reset_parameters()\n",
        "        for module in self.phi_node:\n",
        "            module.reset_parameters()\n",
        "        for module in self.phi_global:\n",
        "            module.reset_parameters()\n",
        "\n",
        "    def propagate(self, *args, **kwargs):\n",
        "        edge_index = kwargs['edge_index']\n",
        "        size = self.__check_input__(edge_index, None)\n",
        "        # Run \"fused\" message and aggregation (if applicable).\n",
        "        if (isinstance(edge_index, SparseTensor) and self.fuse\n",
        "                and not self.__explain__):\n",
        "            raise NotImplementedError\n",
        "            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n",
        "                                         size, kwargs)\n",
        "\n",
        "            msg_aggr_kwargs = self.inspector.distribute(\n",
        "                'message_and_aggregate', coll_dict)\n",
        "            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n",
        "\n",
        "            update_kwargs = self.inspector.distribute('update', coll_dict)\n",
        "            return self.update(out, **update_kwargs)\n",
        "\n",
        "        # Otherwise, run both functions in separation.\n",
        "        elif isinstance(edge_index, Tensor) or not self.fuse:\n",
        "            coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n",
        "                                         kwargs)\n",
        "\n",
        "            msg_kwargs = self.inspector.distribute('message', coll_dict)\n",
        "            out = self.message(**msg_kwargs)\n",
        "            new_edge_attr = out.clone()\n",
        "\n",
        "            # For `GNNExplainer`, we require a separate message and aggregate\n",
        "            # procedure since this allows us to inject the `edge_mask` into the\n",
        "            # message passing computation scheme.\n",
        "            # if self.__explain__:\n",
        "            #     edge_mask = self.__edge_mask__.sigmoid()\n",
        "            #     # Some ops add self-loops to `edge_index`. We need to do the\n",
        "            #     # same for `edge_mask` (but do not train those).\n",
        "            #     if out.size(self.node_dim) != edge_mask.size(0):\n",
        "            #         loop = edge_mask.new_ones(size[0])\n",
        "            #         edge_mask = torch.cat([edge_mask, loop], dim=0)\n",
        "            #     assert out.size(self.node_dim) == edge_mask.size(0)\n",
        "            #     out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))\n",
        "\n",
        "            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n",
        "            out = self.aggregate(out, **aggr_kwargs)\n",
        "\n",
        "            update_kwargs = self.inspector.distribute('update', coll_dict)\n",
        "            edge_embedding = self.update(out, **update_kwargs)\n",
        "            return edge_embedding, new_edge_attr\n",
        "\n",
        "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj, edge_attr: OptTensor, global_attr: Tensor, \n",
        "                node_batch: Tensor, edge_batch: Tensor, size: Size = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        for lin in self.node_lin:\n",
        "            x = F.softplus(lin(x))\n",
        "        \n",
        "        for lin in self.edge_lin:\n",
        "            edge_attr = F.softplus(lin(edge_attr))\n",
        "\n",
        "        for lin in self.global_lin:\n",
        "            global_attr = F.softplus(lin(global_attr))\n",
        "\n",
        "        if isinstance(x, Tensor):\n",
        "            x: PairTensor = (x, x)\n",
        "\n",
        "        edge_embedding, new_edge_attr = self.propagate(edge_index=edge_index, x=x, edge_attr=edge_attr, global_attr=global_attr, size=size, edge_batch=edge_batch)\n",
        "        new_node_attr = torch.cat([x[0], edge_embedding, global_attr[node_batch]], dim=-1)\n",
        "\n",
        "        for module in self.phi_node:\n",
        "            new_node_attr = F.softplus(module(new_node_attr))\n",
        "\n",
        "        new_global_attr = torch.cat([global_mean_pool(new_node_attr, node_batch), global_mean_pool(new_edge_attr, edge_batch), global_attr], dim=-1)\n",
        "\n",
        "        for module in self.phi_global:\n",
        "            new_global_attr = F.softplus(module(new_global_attr))\n",
        "        \n",
        "        new_node_attr = new_node_attr + x[0]\n",
        "        new_edge_attr = new_edge_attr + edge_attr\n",
        "        new_global_attr = new_global_attr + global_attr\n",
        "\n",
        "        return new_node_attr, new_edge_attr, new_global_attr\n",
        "\n",
        "\n",
        "    def message(self, x_i, x_j, edge_attr: OptTensor, global_attr: Tensor, edge_batch:Tensor) -> Tensor:\n",
        "        new_edge_attr = torch.cat([x_i, x_j, edge_attr, global_attr[edge_batch]], dim=-1)\n",
        "        for module in self.phi_edge:\n",
        "            new_edge_attr = F.softplus(module(new_edge_attr))\n",
        "        return new_edge_attr\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(node_channels={}, edge_channels={}, global_channels={})'.format(\n",
        "                self.__class__.__name__, self.node_channels, self.edge_channels, self.global_channels,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MP4p-xYWt37S"
      },
      "outputs": [],
      "source": [
        "node_channels=(dataset.num_node_features, 64, 32)\n",
        "edge_channels=(dataset.num_edge_features, 64, 32)\n",
        "global_channels=(dataset.num_global_features, 64, 32)\n",
        "block = MegNetBlock(node_channels, edge_channels, global_channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmSe5vaDwOvT"
      },
      "source": [
        "## 3. Building MegNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv22E6rjwT1l",
        "outputId": "ede729ae-ec3f-4359-d964-4537dc5a1a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MegNet(\n",
            "  (blocks): ModuleList(\n",
            "    (0): MegNetBlock(node_channels=(16, 16, 9), edge_channels=(10, 16, 10), global_channels=(2, 8, 2))\n",
            "    (1): MegNetBlock(node_channels=(9, 16, 9), edge_channels=(10, 16, 10), global_channels=(2, 8, 2))\n",
            "    (2): MegNetBlock(node_channels=(9, 16, 9), edge_channels=(10, 16, 10), global_channels=(2, 8, 2))\n",
            "  )\n",
            "  (global_node_pool): Set2Set(9, 18)\n",
            "  (global_edge_pool): Set2Set(10, 20)\n",
            "  (out): ModuleList(\n",
            "    (0): Linear(in_features=40, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear, Embedding\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import CGConv\n",
        "from torch.nn import ModuleList, Softplus\n",
        "from torch_geometric.nn import global_mean_pool, Set2Set\n",
        "\n",
        "class MegNet(torch.nn.Module):\n",
        "    def __init__(self, node_channels, edge_channels, global_channels, out_channels,\n",
        "                 pooling_args=(3,), n_blocks=3, batch_norm=False, bias=True):\n",
        "        super(MegNet, self).__init__()\n",
        "        # torch.manual_seed(12345)\n",
        "        self.n_blocks = n_blocks\n",
        "        self.node_channels = node_channels\n",
        "        self.edge_channels = edge_channels\n",
        "        self.global_channels = global_channels\n",
        "        self.pooling_args = pooling_args\n",
        "        self.out_channels = out_channels\n",
        "        self.batch_norm = batch_norm\n",
        "        if batch_norm:\n",
        "            self.bn = ModuleList([BatchNorm1d(node_channels[0]), BatchNorm1d(edge_channels[0]), BatchNorm1d(global_channels[0])])\n",
        "        self.blocks = ModuleList([MegNetBlock(node_channels, edge_channels, global_channels, batch_norm=batch_norm, bias=bias)])\n",
        "        node_channels = (node_channels[2], node_channels[1], node_channels[2])\n",
        "        edge_channels = (edge_channels[2], edge_channels[1], edge_channels[2])\n",
        "        global_channels = (global_channels[2], global_channels[1], global_channels[2])\n",
        "        self.blocks.extend([MegNetBlock(node_channels, edge_channels, global_channels, batch_norm=batch_norm, bias=bias) for i in range(n_blocks-1)])\n",
        "        \n",
        "        self.global_node_pool = Set2Set(node_channels[2], *pooling_args)\n",
        "        self.global_edge_pool = Set2Set(edge_channels[2], *pooling_args)\n",
        "\n",
        "        \n",
        "        \n",
        "        self.out = ModuleList([Linear(2*node_channels[2]+2*edge_channels[2]+global_channels[2], out_channels[0], bias=bias),\n",
        "                               Linear(out_channels[0], out_channels[1], bias=bias),\n",
        "                               Linear(out_channels[1], out_channels[2], bias=bias)])\n",
        "        \n",
        "\n",
        " \n",
        "    def forward(self, x, edge_index, edge_attr, global_attr, node_batch, edge_batch): \n",
        "        if self.batch_norm:\n",
        "            x = self.bn[0](x)\n",
        "            edge_attr = self.bn[1](edge_attr)\n",
        "            global_attr = self.bn[2](global_attr)\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x, edge_attr, global_attr = block(x, edge_index, edge_attr, global_attr, node_batch, edge_batch)\n",
        "            x = F.softplus(x)\n",
        "            edge_attr = F.softplus(edge_attr)\n",
        "            global_attr = F.softplus(global_attr)\n",
        "            \n",
        "\n",
        "        x = self.global_node_pool(x, node_batch)\n",
        "        edge_attr = self.global_edge_pool(edge_attr, edge_batch)\n",
        "\n",
        "        out = torch.cat([x, edge_attr, global_attr], dim=-1)\n",
        "\n",
        "        for module in self.out:\n",
        "            out = F.softplus(module(out))\n",
        "            out = F.dropout(out, training=self.training)\n",
        "    \n",
        "        return out\n",
        "\n",
        "model = MegNet(node_channels=(16, 16, 9), edge_channels=(10, 16, 10), global_channels=(2, 8, 2), out_channels=(32, 16, 1), n_blocks=3)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWMuiIWb3Fpo"
      },
      "source": [
        "## 4. Training MegNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxhdgvNT3Eu1",
        "outputId": "365bc355-9457-405e-ae4b-81834be76bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 2721\n",
            "Number of validation graphs: 340\n",
            "Number of test graphs: 341\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "validation_dataset = dataset[int(0.8 * len(dataset)):int(0.9 * len(dataset))]\n",
        "test_dataset = dataset[int(0.9 * len(dataset)):]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of validation graphs: {len(validation_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=300, num_workers=16, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=100, num_workers=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, num_workers=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5psRL0973W9R",
        "outputId": "c30f619d-876d-46b2-82ff-5c91fa09e40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MegNet(\n",
            "  (blocks): ModuleList(\n",
            "    (0): MegNetBlock(node_channels=(24, 32, 16), edge_channels=(100, 32, 16), global_channels=(33, 32, 16))\n",
            "    (1): MegNetBlock(node_channels=(16, 32, 16), edge_channels=(16, 32, 16), global_channels=(16, 32, 16))\n",
            "    (2): MegNetBlock(node_channels=(16, 32, 16), edge_channels=(16, 32, 16), global_channels=(16, 32, 16))\n",
            "  )\n",
            "  (global_node_pool): Set2Set(16, 32)\n",
            "  (global_edge_pool): Set2Set(16, 32)\n",
            "  (out): ModuleList(\n",
            "    (0): Linear(in_features=80, out_features=16, bias=True)\n",
            "    (1): Linear(in_features=16, out_features=8, bias=True)\n",
            "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = MegNet(node_channels=(dataset.num_node_features, 32, 16), \n",
        "               edge_channels=(dataset.num_edge_features, 32, 16), \n",
        "               global_channels=(dataset.num_global_features, 32, 16), \n",
        "               out_channels=(16, 8, 1), n_blocks=3, batch_norm=False)\n",
        "print(model)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V1QqiSJy3czU",
        "outputId": "8faa7a92-c5d6-4795-801e-73f18293f0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train MAE: 0.8147, Validation MAE: 0.8598\n",
            "Epoch: 002, Train MAE: 0.8261, Validation MAE: 0.8822\n",
            "Epoch: 003, Train MAE: 0.8145, Validation MAE: 0.8062\n",
            "Epoch: 004, Train MAE: 0.7452, Validation MAE: 0.7674\n",
            "Epoch: 005, Train MAE: 0.7670, Validation MAE: 0.7166\n",
            "Epoch: 006, Train MAE: 0.7649, Validation MAE: 0.7825\n",
            "Epoch: 007, Train MAE: 0.6919, Validation MAE: 0.7306\n",
            "Epoch: 008, Train MAE: 0.7355, Validation MAE: 0.6704\n",
            "Epoch: 009, Train MAE: 0.7742, Validation MAE: 0.7639\n",
            "Epoch: 010, Train MAE: 0.7032, Validation MAE: 0.7006\n",
            "Epoch: 011, Train MAE: 0.7715, Validation MAE: 0.7518\n",
            "Epoch: 012, Train MAE: 0.7354, Validation MAE: 0.7125\n",
            "Epoch: 013, Train MAE: 0.6489, Validation MAE: 0.6552\n",
            "Epoch: 014, Train MAE: 0.6461, Validation MAE: 0.6717\n",
            "Epoch: 015, Train MAE: 0.6399, Validation MAE: 0.6127\n",
            "Epoch: 016, Train MAE: 0.5964, Validation MAE: 0.6246\n",
            "Epoch: 017, Train MAE: 0.6228, Validation MAE: 0.6098\n",
            "Epoch: 018, Train MAE: 0.6257, Validation MAE: 0.6503\n",
            "Epoch: 019, Train MAE: 0.6725, Validation MAE: 0.6161\n",
            "Epoch: 020, Train MAE: 0.6176, Validation MAE: 0.6089\n",
            "Epoch: 021, Train MAE: 0.5768, Validation MAE: 0.5867\n",
            "Epoch: 022, Train MAE: 0.6194, Validation MAE: 0.6312\n",
            "Epoch: 023, Train MAE: 0.7303, Validation MAE: 0.8057\n",
            "Epoch: 024, Train MAE: 0.6698, Validation MAE: 0.7095\n",
            "Epoch: 025, Train MAE: 0.6052, Validation MAE: 0.6577\n",
            "Epoch: 026, Train MAE: 0.6057, Validation MAE: 0.5594\n",
            "Epoch: 027, Train MAE: 0.6220, Validation MAE: 0.6245\n",
            "Epoch: 028, Train MAE: 0.5782, Validation MAE: 0.5735\n",
            "Epoch: 029, Train MAE: 0.6089, Validation MAE: 0.6125\n",
            "Epoch: 030, Train MAE: 0.5759, Validation MAE: 0.5855\n",
            "Epoch: 031, Train MAE: 0.5874, Validation MAE: 0.5826\n",
            "Epoch: 032, Train MAE: 0.5824, Validation MAE: 0.5909\n",
            "Epoch: 033, Train MAE: 0.6095, Validation MAE: 0.6248\n",
            "Epoch: 034, Train MAE: 0.5863, Validation MAE: 0.5674\n",
            "Epoch: 035, Train MAE: 0.5799, Validation MAE: 0.5482\n",
            "Epoch: 036, Train MAE: 0.6005, Validation MAE: 0.5557\n",
            "Epoch: 037, Train MAE: 0.5796, Validation MAE: 0.5741\n",
            "Epoch: 038, Train MAE: 0.6023, Validation MAE: 0.5795\n",
            "Epoch: 039, Train MAE: 0.5967, Validation MAE: 0.6067\n",
            "Epoch: 040, Train MAE: 0.5790, Validation MAE: 0.6168\n",
            "Epoch: 041, Train MAE: 0.5596, Validation MAE: 0.5714\n",
            "Epoch: 042, Train MAE: 0.5717, Validation MAE: 0.5970\n",
            "Epoch: 043, Train MAE: 0.5723, Validation MAE: 0.6168\n",
            "Epoch: 044, Train MAE: 0.5578, Validation MAE: 0.5705\n",
            "Epoch: 045, Train MAE: 0.5365, Validation MAE: 0.5556\n",
            "Epoch: 046, Train MAE: 0.5468, Validation MAE: 0.5587\n",
            "Epoch: 047, Train MAE: 0.5388, Validation MAE: 0.5050\n",
            "Epoch: 048, Train MAE: 0.6008, Validation MAE: 0.5660\n",
            "Epoch: 049, Train MAE: 0.5768, Validation MAE: 0.5478\n",
            "Epoch: 050, Train MAE: 0.5951, Validation MAE: 0.5470\n",
            "Epoch: 051, Train MAE: 0.5170, Validation MAE: 0.5401\n",
            "Epoch: 052, Train MAE: 0.5382, Validation MAE: 0.5821\n",
            "Epoch: 053, Train MAE: 0.5346, Validation MAE: 0.5519\n",
            "Epoch: 054, Train MAE: 0.5442, Validation MAE: 0.5636\n",
            "Epoch: 055, Train MAE: 0.5410, Validation MAE: 0.6022\n",
            "Epoch: 056, Train MAE: 0.5629, Validation MAE: 0.6028\n",
            "Epoch: 057, Train MAE: 0.5057, Validation MAE: 0.5601\n",
            "Epoch: 058, Train MAE: 0.5273, Validation MAE: 0.5390\n",
            "Epoch: 059, Train MAE: 0.5556, Validation MAE: 0.5789\n",
            "Epoch: 060, Train MAE: 0.5381, Validation MAE: 0.5564\n",
            "Epoch: 061, Train MAE: 0.5171, Validation MAE: 0.5558\n",
            "Epoch: 062, Train MAE: 0.5300, Validation MAE: 0.5405\n",
            "Epoch: 063, Train MAE: 0.5720, Validation MAE: 0.6278\n",
            "Epoch: 064, Train MAE: 0.5215, Validation MAE: 0.5324\n",
            "Epoch: 065, Train MAE: 0.5110, Validation MAE: 0.5590\n",
            "Epoch: 066, Train MAE: 0.5610, Validation MAE: 0.5581\n",
            "Epoch: 067, Train MAE: 0.5531, Validation MAE: 0.5502\n",
            "Epoch: 068, Train MAE: 0.4806, Validation MAE: 0.5384\n",
            "Epoch: 069, Train MAE: 0.5285, Validation MAE: 0.5627\n",
            "Epoch: 070, Train MAE: 0.5502, Validation MAE: 0.5415\n",
            "Epoch: 071, Train MAE: 0.4998, Validation MAE: 0.5188\n",
            "Epoch: 072, Train MAE: 0.4804, Validation MAE: 0.5380\n",
            "Epoch: 073, Train MAE: 0.4705, Validation MAE: 0.5349\n",
            "Epoch: 074, Train MAE: 0.4993, Validation MAE: 0.5115\n",
            "Epoch: 075, Train MAE: 0.4877, Validation MAE: 0.5020\n",
            "Epoch: 076, Train MAE: 0.5279, Validation MAE: 0.5661\n",
            "Epoch: 077, Train MAE: 0.5175, Validation MAE: 0.5277\n",
            "Epoch: 078, Train MAE: 0.5561, Validation MAE: 0.5398\n",
            "Epoch: 079, Train MAE: 0.5026, Validation MAE: 0.5285\n",
            "Epoch: 080, Train MAE: 0.5245, Validation MAE: 0.5416\n",
            "Epoch: 081, Train MAE: 0.4823, Validation MAE: 0.5081\n",
            "Epoch: 082, Train MAE: 0.5224, Validation MAE: 0.5401\n",
            "Epoch: 083, Train MAE: 0.4909, Validation MAE: 0.5643\n",
            "Epoch: 084, Train MAE: 0.4982, Validation MAE: 0.5103\n",
            "Epoch: 085, Train MAE: 0.4711, Validation MAE: 0.5055\n",
            "Epoch: 086, Train MAE: 0.5145, Validation MAE: 0.5307\n",
            "Epoch: 087, Train MAE: 0.5131, Validation MAE: 0.5608\n",
            "Epoch: 088, Train MAE: 0.5509, Validation MAE: 0.5153\n",
            "Epoch: 089, Train MAE: 0.5369, Validation MAE: 0.5138\n",
            "Epoch: 090, Train MAE: 0.5329, Validation MAE: 0.5224\n",
            "Epoch: 091, Train MAE: 0.4820, Validation MAE: 0.5199\n",
            "Epoch: 092, Train MAE: 0.5401, Validation MAE: 0.5483\n",
            "Epoch: 093, Train MAE: 0.5003, Validation MAE: 0.5532\n",
            "Epoch: 094, Train MAE: 0.5319, Validation MAE: 0.5366\n",
            "Epoch: 095, Train MAE: 0.5544, Validation MAE: 0.5383\n",
            "Epoch: 096, Train MAE: 0.5175, Validation MAE: 0.5258\n",
            "Epoch: 097, Train MAE: 0.5552, Validation MAE: 0.5922\n",
            "Epoch: 098, Train MAE: 0.5401, Validation MAE: 0.5253\n",
            "Epoch: 099, Train MAE: 0.5177, Validation MAE: 0.5206\n",
            "Epoch: 100, Train MAE: 0.5321, Validation MAE: 0.5289\n",
            "Epoch: 101, Train MAE: 0.5149, Validation MAE: 0.5536\n",
            "Epoch: 102, Train MAE: 0.5167, Validation MAE: 0.4936\n",
            "Epoch: 103, Train MAE: 0.5316, Validation MAE: 0.5313\n",
            "Epoch: 104, Train MAE: 0.5313, Validation MAE: 0.5371\n",
            "Epoch: 105, Train MAE: 0.5219, Validation MAE: 0.5421\n",
            "Epoch: 106, Train MAE: 0.5616, Validation MAE: 0.5510\n",
            "Epoch: 107, Train MAE: 0.4885, Validation MAE: 0.5149\n",
            "Epoch: 108, Train MAE: 0.5033, Validation MAE: 0.5099\n",
            "Epoch: 109, Train MAE: 0.4963, Validation MAE: 0.5124\n",
            "Epoch: 110, Train MAE: 0.5179, Validation MAE: 0.5155\n",
            "Epoch: 111, Train MAE: 0.5021, Validation MAE: 0.5488\n",
            "Epoch: 112, Train MAE: 0.5543, Validation MAE: 0.5765\n",
            "Epoch: 113, Train MAE: 0.5170, Validation MAE: 0.6073\n",
            "Epoch: 114, Train MAE: 0.4991, Validation MAE: 0.4871\n",
            "Epoch: 115, Train MAE: 0.5208, Validation MAE: 0.5417\n",
            "Epoch: 116, Train MAE: 0.5137, Validation MAE: 0.5179\n",
            "Epoch: 117, Train MAE: 0.6104, Validation MAE: 0.5991\n",
            "Epoch: 118, Train MAE: 0.5535, Validation MAE: 0.5304\n",
            "Epoch: 119, Train MAE: 0.5306, Validation MAE: 0.5694\n",
            "Epoch: 120, Train MAE: 0.5220, Validation MAE: 0.4979\n",
            "Epoch: 121, Train MAE: 0.4868, Validation MAE: 0.5386\n",
            "Epoch: 122, Train MAE: 0.5216, Validation MAE: 0.5293\n",
            "Epoch: 123, Train MAE: 0.5032, Validation MAE: 0.5470\n",
            "Epoch: 124, Train MAE: 0.5145, Validation MAE: 0.4882\n",
            "Epoch: 125, Train MAE: 0.5111, Validation MAE: 0.5481\n",
            "Epoch: 126, Train MAE: 0.5507, Validation MAE: 0.5380\n",
            "Epoch: 127, Train MAE: 0.4729, Validation MAE: 0.5203\n",
            "Epoch: 128, Train MAE: 0.5309, Validation MAE: 0.5195\n",
            "Epoch: 129, Train MAE: 0.5392, Validation MAE: 0.5065\n",
            "Epoch: 130, Train MAE: 0.4893, Validation MAE: 0.4848\n",
            "Epoch: 131, Train MAE: 0.5085, Validation MAE: 0.5247\n",
            "Epoch: 132, Train MAE: 0.5058, Validation MAE: 0.5047\n",
            "Epoch: 133, Train MAE: 0.4732, Validation MAE: 0.4962\n",
            "Epoch: 134, Train MAE: 0.5127, Validation MAE: 0.5272\n",
            "Epoch: 135, Train MAE: 0.5020, Validation MAE: 0.5415\n",
            "Epoch: 136, Train MAE: 0.4978, Validation MAE: 0.5137\n",
            "Epoch: 137, Train MAE: 0.5094, Validation MAE: 0.4881\n",
            "Epoch: 138, Train MAE: 0.5082, Validation MAE: 0.5311\n",
            "Epoch: 139, Train MAE: 0.4753, Validation MAE: 0.5022\n",
            "Epoch: 140, Train MAE: 0.4729, Validation MAE: 0.4551\n",
            "Epoch: 141, Train MAE: 0.5466, Validation MAE: 0.5091\n",
            "Epoch: 142, Train MAE: 0.5595, Validation MAE: 0.5216\n",
            "Epoch: 143, Train MAE: 0.5109, Validation MAE: 0.5119\n",
            "Epoch: 144, Train MAE: 0.5160, Validation MAE: 0.5336\n",
            "Epoch: 145, Train MAE: 0.5117, Validation MAE: 0.5050\n",
            "Epoch: 146, Train MAE: 0.4791, Validation MAE: 0.5131\n",
            "Epoch: 147, Train MAE: 0.5072, Validation MAE: 0.5143\n",
            "Epoch: 148, Train MAE: 0.5095, Validation MAE: 0.5066\n",
            "Epoch: 149, Train MAE: 0.5548, Validation MAE: 0.4865\n",
            "Epoch: 150, Train MAE: 0.4907, Validation MAE: 0.5409\n",
            "Epoch: 151, Train MAE: 0.5140, Validation MAE: 0.5275\n",
            "Epoch: 152, Train MAE: 0.4795, Validation MAE: 0.4960\n",
            "Epoch: 153, Train MAE: 0.4944, Validation MAE: 0.4781\n",
            "Epoch: 154, Train MAE: 0.4745, Validation MAE: 0.4874\n",
            "Epoch: 155, Train MAE: 0.4887, Validation MAE: 0.5483\n",
            "Epoch: 156, Train MAE: 0.4754, Validation MAE: 0.4526\n",
            "Epoch: 157, Train MAE: 0.4753, Validation MAE: 0.5239\n",
            "Epoch: 158, Train MAE: 0.5205, Validation MAE: 0.4957\n",
            "Epoch: 159, Train MAE: 0.4958, Validation MAE: 0.5260\n",
            "Epoch: 160, Train MAE: 0.4764, Validation MAE: 0.5256\n",
            "Epoch: 161, Train MAE: 0.4611, Validation MAE: 0.5335\n",
            "Epoch: 162, Train MAE: 0.4848, Validation MAE: 0.4948\n",
            "Epoch: 163, Train MAE: 0.4815, Validation MAE: 0.4917\n",
            "Epoch: 164, Train MAE: 0.4987, Validation MAE: 0.5174\n",
            "Epoch: 165, Train MAE: 0.4618, Validation MAE: 0.5127\n",
            "Epoch: 166, Train MAE: 0.5071, Validation MAE: 0.5844\n",
            "Epoch: 167, Train MAE: 0.4941, Validation MAE: 0.5134\n",
            "Epoch: 168, Train MAE: 0.5308, Validation MAE: 0.5393\n",
            "Epoch: 169, Train MAE: 0.4657, Validation MAE: 0.4989\n",
            "Epoch: 170, Train MAE: 0.5114, Validation MAE: 0.4973\n",
            "Epoch: 171, Train MAE: 0.7745, Validation MAE: 0.7672\n",
            "Epoch: 172, Train MAE: 0.7165, Validation MAE: 0.6323\n",
            "Epoch: 173, Train MAE: 0.6514, Validation MAE: 0.6866\n",
            "Epoch: 174, Train MAE: 0.6305, Validation MAE: 0.6463\n",
            "Epoch: 175, Train MAE: 0.7180, Validation MAE: 0.6972\n",
            "Epoch: 176, Train MAE: 0.5852, Validation MAE: 0.5828\n",
            "Epoch: 177, Train MAE: 0.5656, Validation MAE: 0.5849\n",
            "Epoch: 178, Train MAE: 0.6733, Validation MAE: 0.6452\n",
            "Epoch: 179, Train MAE: 0.5657, Validation MAE: 0.5723\n",
            "Epoch: 180, Train MAE: 0.6469, Validation MAE: 0.6506\n",
            "Epoch: 181, Train MAE: 0.5912, Validation MAE: 0.5946\n",
            "Epoch: 182, Train MAE: 0.5660, Validation MAE: 0.5729\n",
            "Epoch: 183, Train MAE: 0.5756, Validation MAE: 0.5772\n",
            "Epoch: 184, Train MAE: 0.5622, Validation MAE: 0.5856\n",
            "Epoch: 185, Train MAE: 0.5859, Validation MAE: 0.6185\n",
            "Epoch: 186, Train MAE: 0.5854, Validation MAE: 0.5350\n",
            "Epoch: 187, Train MAE: 0.5833, Validation MAE: 0.6380\n",
            "Epoch: 188, Train MAE: 0.5695, Validation MAE: 0.5726\n",
            "Epoch: 189, Train MAE: 0.5693, Validation MAE: 0.5816\n",
            "Epoch: 190, Train MAE: 0.5932, Validation MAE: 0.5735\n",
            "Epoch: 191, Train MAE: 0.5623, Validation MAE: 0.5749\n",
            "Epoch: 192, Train MAE: 0.5383, Validation MAE: 0.5781\n",
            "Epoch: 193, Train MAE: 0.5900, Validation MAE: 0.5951\n",
            "Epoch: 194, Train MAE: 0.6099, Validation MAE: 0.6084\n",
            "Epoch: 195, Train MAE: 0.5533, Validation MAE: 0.5938\n",
            "Epoch: 196, Train MAE: 0.5561, Validation MAE: 0.5567\n",
            "Epoch: 197, Train MAE: 0.5693, Validation MAE: 0.5283\n",
            "Epoch: 198, Train MAE: 0.5620, Validation MAE: 0.5681\n",
            "Epoch: 199, Train MAE: 0.5668, Validation MAE: 0.5942\n",
            "Epoch: 200, Train MAE: 0.5558, Validation MAE: 0.5850\n",
            "Epoch: 201, Train MAE: 0.5672, Validation MAE: 0.5786\n",
            "Epoch: 202, Train MAE: 0.5592, Validation MAE: 0.5567\n",
            "Epoch: 203, Train MAE: 0.5415, Validation MAE: 0.5767\n",
            "Epoch: 204, Train MAE: 0.6208, Validation MAE: 0.5895\n",
            "Epoch: 205, Train MAE: 0.5903, Validation MAE: 0.5811\n",
            "Epoch: 206, Train MAE: 0.6247, Validation MAE: 0.5582\n",
            "Epoch: 207, Train MAE: 0.6365, Validation MAE: 0.5833\n",
            "Epoch: 208, Train MAE: 0.5946, Validation MAE: 0.6072\n",
            "Epoch: 209, Train MAE: 0.6179, Validation MAE: 0.5839\n",
            "Epoch: 210, Train MAE: 0.5960, Validation MAE: 0.5914\n",
            "Epoch: 211, Train MAE: 0.6708, Validation MAE: 0.5858\n",
            "Epoch: 212, Train MAE: 0.5907, Validation MAE: 0.5863\n",
            "Epoch: 213, Train MAE: 0.5592, Validation MAE: 0.5771\n",
            "Epoch: 214, Train MAE: 0.5712, Validation MAE: 0.5764\n",
            "Epoch: 215, Train MAE: 0.6549, Validation MAE: 0.6536\n",
            "Epoch: 216, Train MAE: 0.5935, Validation MAE: 0.5949\n",
            "Epoch: 217, Train MAE: 0.5916, Validation MAE: 0.5835\n",
            "Epoch: 218, Train MAE: 0.5455, Validation MAE: 0.5268\n",
            "Epoch: 219, Train MAE: 0.5497, Validation MAE: 0.5809\n",
            "Epoch: 220, Train MAE: 0.5822, Validation MAE: 0.5991\n",
            "Epoch: 221, Train MAE: 0.6231, Validation MAE: 0.5665\n",
            "Epoch: 222, Train MAE: 0.5942, Validation MAE: 0.5902\n",
            "Epoch: 223, Train MAE: 0.6076, Validation MAE: 0.5718\n",
            "Epoch: 224, Train MAE: 0.5547, Validation MAE: 0.5824\n",
            "Epoch: 225, Train MAE: 0.5577, Validation MAE: 0.5854\n",
            "Epoch: 226, Train MAE: 0.5911, Validation MAE: 0.5822\n",
            "Epoch: 227, Train MAE: 0.5845, Validation MAE: 0.5820\n",
            "Epoch: 228, Train MAE: 0.5767, Validation MAE: 0.5845\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-afc09310b4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#print(f'Memory allocated, kB: {torch.cuda.memory_allocated()/1000}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-afc09310b4bd>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training/test dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnode_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:  # Iterate in batches over the training dataset.\n",
        "        batch.to(device)\n",
        "        node_batch = batch.batch\n",
        "        edge_batch = torch.cat([torch.tensor([i]).repeat(batch[i].num_edges) for i in range(batch.num_graphs)]).to(device)\n",
        "        out = model(batch.x, batch.edge_index, batch.edge_attr, batch.global_attr, node_batch, edge_batch)  # Perform a single forward pass.\n",
        "        loss = criterion(out, batch.y.view(*out.shape))  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def validate(loader):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss = 0\n",
        "        for batch in loader:  # Iterate in batches over the training/test dataset.\n",
        "            batch.to(device)\n",
        "            node_batch = batch.batch\n",
        "            edge_batch = torch.cat([torch.tensor([i]).repeat(batch[i].num_edges) for i in range(batch.num_graphs)]).to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.global_attr, node_batch, edge_batch)  \n",
        "            #loss = criterion(out, batch.y.view(*out.shape))\n",
        "            loss += torch.abs(out - batch.y.view(*out.shape)).mean()\n",
        "        return loss / len(loader)\n",
        "\n",
        "for epoch in range(1, 1000):\n",
        "    train()\n",
        "    train_acc = validate(train_loader)\n",
        "    valid_acc = validate(validation_loader)\n",
        "    #print(f'Memory allocated, kB: {torch.cuda.memory_allocated()/1000}')\n",
        "    #print(f'Memory cached, kB: {torch.cuda.memory_cached()/1000}')\n",
        "    print(f'Epoch: {epoch:03d}, Train MAE: {train_acc:.4f}, Validation MAE: {valid_acc:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mmSe5vaDwOvT"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}